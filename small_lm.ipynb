{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 11:56:57.922916: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-01 11:56:57.949895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740815817.981990   25965 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740815817.991421   25965 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-01 11:56:58.022384: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import (\n",
    "    TransformerConfig,\n",
    "    TransformerLM\n",
    ")\n",
    "from trainer import (\n",
    "    create_learning_rate_schedule,\n",
    "    TrainState,\n",
    "    _to_array,\n",
    "    compute_weighted_cross_entropy,\n",
    "    compute_metrics\n",
    ")\n",
    "from steps import train_step\n",
    "import optax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "from jax import random\n",
    "from clu import metric_writers, periodic_actions\n",
    "from absl import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_fn = create_learning_rate_schedule(\n",
    "    learning_rate=0.01, warmup_steps=100\n",
    ")\n",
    "tx = optax.adamw(learning_rate_fn, b1=0.9, b2=0.98, eps=1e-9, weight_decay=0.01)\n",
    "\n",
    "config = TransformerConfig(vocab_size=30728)\n",
    "model = TransformerLM(config, rngs=nnx.Rngs(0))\n",
    "\n",
    "graphdef, params, rest = nnx.split(model, nnx.Param, ...)\n",
    "state = TrainState.create(\n",
    "    apply_fn=graphdef.apply, params=params, tx=tx, graphdef=graphdef, rest=rest\n",
    ")\n",
    "\n",
    "state = jax.tree.map(_to_array, state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_train_step = jax.jit(train_step, static_argnums=(2, 3), donate_argnums=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_step = 0\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "rng, inference_rng = random.split(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = metric_writers.create_default_writer(\n",
    "    './', just_logging=jax.process_index() > 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rngs = rng\n",
    "\n",
    "logging.info('Starting training loop.')\n",
    "hooks = []\n",
    "\n",
    "report_progress = periodic_actions.ReportProgress(\n",
    "    num_train_steps=10, writer=writer\n",
    ")\n",
    "\n",
    "if jax.process_index() == 0:\n",
    "    hooks += [\n",
    "        report_progress,\n",
    "        periodic_actions.Profile(logdir='./', num_profile_steps=5),\n",
    "    ]\n",
    "\n",
    "train_metrics = []\n",
    "\n",
    "with metric_writers.ensure_flushes(writer):\n",
    "    for step in range(0, 100):\n",
    "        with jax.profiler.StepTraceAnnotation('train', step_num=step):\n",
    "            rng, _ = jax.random.split(rng)\n",
    "            batch = jax.random.randint(rng, (3, 64), minval=0, maxval=70_000)\n",
    "            state, metrics = jit_train_step(\n",
    "                state, batch, learning_rate_fn, 0.0, dropout_rngs\n",
    "            )\n",
    "            train_metrics.append(metrics)\n",
    "\n",
    "            # Quick indication that training is happening.\n",
    "            logging.log_first_n(logging.INFO, 'Finished training step %d.', 5, step)\n",
    "            for h in hooks:\n",
    "                h(step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
