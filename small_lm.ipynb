{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import typing as tp\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import lax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class TransformerConfig:\n",
    "    vocab_size: int\n",
    "    logits_via_embedding: bool = False\n",
    "    dtype: tp.Any = jnp.float32\n",
    "    emb_dim: int = 512\n",
    "    num_heads: int = 8\n",
    "    num_layers: int = 3\n",
    "    qkv_dim: int = 512\n",
    "    mlp_dim: int = 2048\n",
    "    max_len: int = 2048\n",
    "    dropout_rate: float = 0.1\n",
    "    attention_dropout_rate: float = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_init(max_len=2048, min_scale=1.0, max_scale=10000.0):\n",
    "  def init(key, shape, dtype=np.float32):\n",
    "    del key, dtype\n",
    "    d_feature = shape[-1]\n",
    "    pe = np.zeros((max_len, d_feature), dtype=np.float32)\n",
    "    position = np.arange(0, max_len)[:, np.newaxis]\n",
    "    scale_factor = -np.log(max_scale / min_scale) / (d_feature // 2 - 1)\n",
    "    div_term = min_scale * np.exp(np.arange(0, d_feature // 2) * scale_factor)\n",
    "    pe[:, : d_feature // 2] = np.sin(position * div_term)\n",
    "    pe[:, d_feature // 2 : 2 * (d_feature // 2)] = np.cos(position * div_term)\n",
    "    pe = pe[np.newaxis, :, :]  # [1, max_len, d_feature]\n",
    "    return jnp.array(pe)\n",
    "\n",
    "  return init\n",
    "\n",
    "def shift_right(x: jax.Array, axis: int = 1):\n",
    "  \"\"\"Shift the input to the right by padding and slicing on axis.\"\"\"\n",
    "  pad_widths: list[tuple[int, int]] = [(0, 0)] * len(x.shape)\n",
    "  pad_widths[axis] = (1, 0)\n",
    "  padded = jnp.pad(\n",
    "    x, pad_widths, mode='constant', constant_values=x.dtype.type(0)\n",
    "  )\n",
    "  return lax.dynamic_slice_in_dim(padded, 0, padded.shape[axis] - 1, axis)\n",
    "\n",
    "def shift_inputs(x: jax.Array, axis: int = 1):\n",
    "  \"\"\"Shift inputs and replace EOS by 0 for packed inputs.\"\"\"\n",
    "  shifted = shift_right(x, axis=axis)\n",
    "\n",
    "  return shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPositionalEmbs(nnx.Module):\n",
    "    def __init__(self, config, *, decode=False, rngs):\n",
    "        self.config = config\n",
    "        self.decode = decode\n",
    "        self.init_func = sinusoidal_init(config.max_len)\n",
    "        self.pos_embedding = self.init_func(rngs.params(), (config.max_len, config.emb_dim))\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        length = inputs.shape[1]\n",
    "        if self.decode:\n",
    "            _, _, df = self.pos_embedding.shape\n",
    "            pos_embedding = lax.dynamic_slice(\n",
    "                self.pos_embedding, jnp.array((0, self.cache_index.value, 0)), (1, 1, df)\n",
    "            )\n",
    "        else:\n",
    "            pos_embedding = self.pos_embedding[:, :length, :]\n",
    "\n",
    "        return inputs + pos_embedding\n",
    "    \n",
    "    def init_cache(self, input_shape, dtype = jnp.float32):\n",
    "        self.cache_index = nnx.Cache(jnp.array(0, dtype=jnp.uint32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpBlock(nnx.Module):\n",
    "    def __init__(self, config, rngs):\n",
    "        self.config = config\n",
    "        self.linear1 = nnx.Linear(config.emb_dim, config.mlp_dim, rngs=rngs)\n",
    "        self.linear2 = nnx.Linear(config.mlp_dim, config.emb_dim, rngs=rngs)\n",
    "        self.dropout = nnx.Dropout(rate=config.dropout_rate)\n",
    "\n",
    "    def __call__(self, inputs, rngs):\n",
    "        x = self.linear1(inputs)\n",
    "        x = nnx.relu(x)\n",
    "        x = self.dropout(x, rngs=rngs)\n",
    "        output = self.linear2(x)\n",
    "        output = self.dropout(output, rngs=rngs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderBlock(nnx.Module):\n",
    "    def __init__(self, config, rngs):\n",
    "        self.config = config\n",
    "        self.ln1 = nnx.LayerNorm(num_features=config.emb_dim, rngs=rngs)\n",
    "        self.ln2 = nnx.LayerNorm(num_features=config.emb_dim, rngs=rngs)\n",
    "        self.attention = nnx.MultiHeadAttention(\n",
    "            num_heads=config.num_heads,\n",
    "            in_features=config.emb_dim,\n",
    "            qkv_features=config.qkv_dim,\n",
    "            use_bias=False,\n",
    "            broadcast_dropout=False,\n",
    "            dropout_rate=config.attention_dropout_rate,\n",
    "            rngs=rngs,\n",
    "            )\n",
    "        self.mlp = MlpBlock(config=config, rngs=rngs)\n",
    "        self.dropout = nnx.Dropout(rate=config.dropout_rate)\n",
    "        \n",
    "    def __call__(self, inputs, rngs, decoder_mask):\n",
    "        x = self.ln1(inputs)\n",
    "        x = self.attention(x, rngs=rngs, mask=decoder_mask)\n",
    "        x = self.dropout(x, rngs=rngs)\n",
    "        x = x + inputs\n",
    "        z = self.ln2(x)\n",
    "        z = self.mlp(z, rngs)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nnx.Module):\n",
    "    def __init__(self, config, *, decode = False, rngs):\n",
    "        self.config = config\n",
    "        self.decode = decode\n",
    "        self.output_embed = nnx.Embed(num_embeddings=config.vocab_size, \n",
    "                                      features=config.emb_dim,\n",
    "                                      rngs=rngs)\n",
    "        self.posembed_out = AddPositionalEmbs(config=config, rngs=rngs)\n",
    "        self.dropout = nnx.Dropout(rate=config.dropout_rate)\n",
    "\n",
    "        for idx in range(config.num_layers):\n",
    "            layer = EncoderDecoderBlock(config,rngs)\n",
    "            setattr(self, f'encoderdecoderblock_{idx}', layer)\n",
    "\n",
    "        self.encoderdecoder_norm = nnx.LayerNorm(num_features=config.emb_dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, inputs, rngs, decoder_mask=None):\n",
    "        y = inputs.astype('int32')\n",
    "        if not self.decode:\n",
    "            y = shift_inputs(y)\n",
    "        y = self.output_embed(inputs)\n",
    "        y = self.posembed_out(y)\n",
    "        y = self.dropout(y, rngs=rngs)\n",
    "\n",
    "        for idx in range(self.config.num_layers):\n",
    "            layer = getattr(self, f'encoderdecoderblock_{idx}')\n",
    "            y = layer(y, rngs=rngs, decoder_mask=decoder_mask)\n",
    "\n",
    "        y = self.encoderdecoder_norm(y)\n",
    "\n",
    "        logits = self.output_embed.attend(y)\n",
    "        logits = logits / jnp.sqrt(y.shape[-1])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLM(nnx.Module):\n",
    "    def __init__(self, config, *, decode=False, rngs):\n",
    "        self.config = config\n",
    "        self.decode = decode\n",
    "        self.decoder = Decoder(config, rngs=rngs)\n",
    "\n",
    "    def __call__(self, inputs, rngs):\n",
    "        if self.decode:\n",
    "            decoder_mask = None\n",
    "        else:\n",
    "            decoder_mask = nnx.combine_masks(\n",
    "                nnx.make_attention_mask(inputs > 0, inputs > 0, dtype=self.config.dtype),\n",
    "                nnx.make_causal_mask(inputs, dtype=self.config.dtype),\n",
    "            )\n",
    "            \n",
    "        logits = self.decoder(inputs, rngs, decoder_mask=decoder_mask)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformerConfig(vocab_size=30_728)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerLM(config, rngs=rngs)\n",
    "\n",
    "transformer.set_attributes(deterministic=False, decode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Protocol, runtime_checkable\n",
    "\n",
    "@runtime_checkable\n",
    "class HasCache(Protocol):\n",
    "  def init_cache(self, input_shape, dtype): ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = jax.random.randint(rngs.params(), (1, 1), minval=0, maxval=config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _path, m in transformer.iter_modules():\n",
    "    if isinstance(m, HasCache):\n",
    "      input_shape = (inputs.shape[0], config.max_len, config.emb_dim)\n",
    "      m.init_cache(input_shape, dtype=config.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[ 0.00925957, -0.02592195, -0.02881995, ..., -0.00104929,\n",
       "         -0.00665741,  0.06749014]]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer(inputs, rngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Arrays leaves are not supported, at 'decoder/posembed_out/pos_embedding': [[[ 0.          0.          0.         ...  1.          1.\n    1.        ]\n  [ 0.84147096  0.82177866  0.8018049  ...  1.          1.\n    1.        ]\n  [ 0.9092974   0.9365102   0.9582946  ...  1.          1.\n    1.        ]\n  ...\n  [ 0.17589758 -0.44885764 -0.96926904 ...  0.9759369   0.97760755\n    0.97916263]\n  [-0.7333133   0.47858194 -0.381975   ...  0.9759134   0.97758573\n    0.9791423 ]\n  [-0.9683193   0.9942562   0.51274335 ...  0.97589     0.9775639\n    0.979122  ]]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCache\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/microlm/lib/python3.12/site-packages/flax/nnx/graph.py:1475\u001b[0m, in \u001b[0;36mstate\u001b[0;34m(node, *filters)\u001b[0m\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstate\u001b[39m(\n\u001b[1;32m   1443\u001b[0m   node,\n\u001b[1;32m   1444\u001b[0m   \u001b[38;5;241m*\u001b[39mfilters: filterlib\u001b[38;5;241m.\u001b[39mFilter,\n\u001b[1;32m   1445\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tp\u001b[38;5;241m.\u001b[39mUnion[GraphState, \u001b[38;5;28mtuple\u001b[39m[GraphState, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]:\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Similar to :func:`split` but only returns the :class:`State`'s indicated by the filters.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[1;32m   1448\u001b[0m \u001b[38;5;124;03m  Example usage::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;124;03m    One or more :class:`State` mappings.\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1475\u001b[0m   _, state \u001b[38;5;241m=\u001b[39m \u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1477\u001b[0m   states: GraphState \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m[GraphState, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m   1478\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filters) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/micromamba/envs/microlm/lib/python3.12/site-packages/flax/nnx/graph.py:404\u001b[0m, in \u001b[0;36mflatten\u001b[0;34m(node, ref_index)\u001b[0m\n\u001b[1;32m    402\u001b[0m   ref_index \u001b[38;5;241m=\u001b[39m RefMap()\n\u001b[1;32m    403\u001b[0m flat_state: \u001b[38;5;28mdict\u001b[39m[PathParts, StateLeaf] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 404\u001b[0m graphdef \u001b[38;5;241m=\u001b[39m \u001b[43m_graph_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graphdef, GraphState\u001b[38;5;241m.\u001b[39mfrom_flat_path(flat_state)\n",
      "File \u001b[0;32m~/micromamba/envs/microlm/lib/python3.12/site-packages/flax/nnx/graph.py:436\u001b[0m, in \u001b[0;36m_graph_flatten\u001b[0;34m(path, ref_index, flat_state, node)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m values:\n\u001b[1;32m    435\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m is_node(value):\n\u001b[0;32m--> 436\u001b[0m     nodedef \u001b[38;5;241m=\u001b[39m \u001b[43m_graph_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m     subgraphs\u001b[38;5;241m.\u001b[39mappend((key, nodedef))\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Variable):\n",
      "File \u001b[0;32m~/micromamba/envs/microlm/lib/python3.12/site-packages/flax/nnx/graph.py:436\u001b[0m, in \u001b[0;36m_graph_flatten\u001b[0;34m(path, ref_index, flat_state, node)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m values:\n\u001b[1;32m    435\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m is_node(value):\n\u001b[0;32m--> 436\u001b[0m     nodedef \u001b[38;5;241m=\u001b[39m \u001b[43m_graph_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m     subgraphs\u001b[38;5;241m.\u001b[39mappend((key, nodedef))\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Variable):\n",
      "File \u001b[0;32m~/micromamba/envs/microlm/lib/python3.12/site-packages/flax/nnx/graph.py:451\u001b[0m, in \u001b[0;36m_graph_flatten\u001b[0;34m(path, ref_index, flat_state, node)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, (jax\u001b[38;5;241m.\u001b[39mArray, np\u001b[38;5;241m.\u001b[39mndarray)):\n\u001b[1;32m    450\u001b[0m       path_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, (\u001b[38;5;241m*\u001b[39mpath, key)))\n\u001b[0;32m--> 451\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    452\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArrays leaves are not supported, at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_str\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    453\u001b[0m       )\n\u001b[1;32m    454\u001b[0m     static_fields\u001b[38;5;241m.\u001b[39mappend((key, value))\n\u001b[1;32m    456\u001b[0m nodedef \u001b[38;5;241m=\u001b[39m NodeDef\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    457\u001b[0m   \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mnode_impl\u001b[38;5;241m.\u001b[39mtype,\n\u001b[1;32m    458\u001b[0m   index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    464\u001b[0m   index_mapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    465\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Arrays leaves are not supported, at 'decoder/posembed_out/pos_embedding': [[[ 0.          0.          0.         ...  1.          1.\n    1.        ]\n  [ 0.84147096  0.82177866  0.8018049  ...  1.          1.\n    1.        ]\n  [ 0.9092974   0.9365102   0.9582946  ...  1.          1.\n    1.        ]\n  ...\n  [ 0.17589758 -0.44885764 -0.96926904 ...  0.9759369   0.97760755\n    0.97916263]\n  [-0.7333133   0.47858194 -0.381975   ...  0.9759134   0.97758573\n    0.9791423 ]\n  [-0.9683193   0.9942562   0.51274335 ...  0.97589     0.9775639\n    0.979122  ]]]"
     ]
    }
   ],
   "source": [
    "nnx.state(transformer, nnx.Cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(300):\n",
    "    inputs = jax.random.randint(rngs.params(), (6, config.max_len), minval=0, maxval=config.vocab_size)\n",
    "\n",
    "    output = transformer(inputs, rngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnx.displasy(transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nnx.Embed(\n",
    "    num_embeddings=30_000,\n",
    "    features=128,\n",
    "    embedding_init=nnx.initializers.normal(stddev=1.0),\n",
    "    rngs=rngs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = jax.random.uniform(rngs.params(), (128, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0.69756776, -0.8900631 ,  0.10091793, ..., -0.8226665 ,\n",
       "        0.25933772, -0.23882598], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.attend(y) / jnp.sqrt(y.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = jnp.array([[1, 2, 3, 4, 1, 0, 0, 0, 0], [1, 2, 3, 4, 5, 0, 0, 0, 0] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1, 2, 3, 4, 1, 0, 0, 0, 0],\n",
       "       [1, 2, 3, 4, 5, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnx.make_causal_mask(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnx.combine_masks(\n",
    "        nnx.make_attention_mask(inputs > 0, inputs > 0),\n",
    "        nnx.make_causal_mask(inputs),\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
