{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 16:41:33.821245: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-01 16:41:33.831930: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740832893.845593  346930 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740832893.849885  346930 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-01 16:41:33.864138: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import (\n",
    "    TransformerConfig,\n",
    "    TransformerLM\n",
    ")\n",
    "from trainer import (\n",
    "    create_learning_rate_schedule,\n",
    "    TrainState,\n",
    "    _to_array,\n",
    "    compute_weighted_cross_entropy,\n",
    "    compute_metrics\n",
    ")\n",
    "from steps import train_step\n",
    "import optax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "from jax import random\n",
    "from clu import metric_writers, periodic_actions\n",
    "from absl import logging\n",
    "from flax.training import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_fn = create_learning_rate_schedule(\n",
    "    learning_rate=0.0016, warmup_steps=100\n",
    ")\n",
    "tx = optax.adamw(learning_rate_fn, b1=0.9, b2=0.98, eps=1e-9, weight_decay=0.1)\n",
    "\n",
    "config = TransformerConfig(vocab_size=30728)\n",
    "model = TransformerLM(config, rngs=nnx.Rngs(0))\n",
    "\n",
    "graphdef, params, rest = nnx.split(model, nnx.Param, ...)\n",
    "state = TrainState.create(\n",
    "    apply_fn=graphdef.apply, params=params, tx=tx, graphdef=graphdef, rest=rest\n",
    ")\n",
    "\n",
    "state = jax.tree.map(_to_array, state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_train_step = jax.jit(train_step, static_argnums=(2, 3), donate_argnums=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_step = 0\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "rng, inference_rng = random.split(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = metric_writers.create_default_writer(\n",
    "    './', just_logging=jax.process_index() > 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 5747/100000 [01:06<18:06, 86.78it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m     rng, _ \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng)\n\u001b[1;32m     22\u001b[0m     batch \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(rng, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m64\u001b[39m), minval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, maxval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m70_000\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m     state, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mjit_train_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rngs\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     train_metrics\u001b[38;5;241m.\u001b[39mappend(metrics)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Quick indication that training is happening.\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/jax_env/lib/python3.12/site-packages/flax/nnx/variablelib.py:887\u001b[0m, in \u001b[0;36m_variable_state_unflatten\u001b[0;34m(static, children)\u001b[0m\n\u001b[1;32m    882\u001b[0m     node \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m    884\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (node,), (x\u001b[38;5;241m.\u001b[39mtype, metadata)\n\u001b[0;32m--> 887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_variable_state_unflatten\u001b[39m(\n\u001b[1;32m    888\u001b[0m   static: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mtype\u001b[39m[Variable[A]], \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, tp\u001b[38;5;241m.\u001b[39mAny], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]],\n\u001b[1;32m    889\u001b[0m   children: \u001b[38;5;28mtuple\u001b[39m[A],\n\u001b[1;32m    890\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VariableState[A]:\n\u001b[1;32m    891\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m VariableState(\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mstatic[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    893\u001b[0m     value\u001b[38;5;241m=\u001b[39mchildren[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(static[\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m    895\u001b[0m   )\n\u001b[1;32m    898\u001b[0m jtu\u001b[38;5;241m.\u001b[39mregister_pytree_with_keys(\n\u001b[1;32m    899\u001b[0m   VariableState,\n\u001b[1;32m    900\u001b[0m   partial(_variable_state_flatten, with_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    901\u001b[0m   _variable_state_unflatten,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    902\u001b[0m   flatten_func\u001b[38;5;241m=\u001b[39mpartial(_variable_state_flatten, with_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    903\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dropout_rngs = rng\n",
    "\n",
    "logging.info('Starting training loop.')\n",
    "hooks = []\n",
    "\n",
    "report_progress = periodic_actions.ReportProgress(\n",
    "    num_train_steps=10, writer=writer\n",
    ")\n",
    "\n",
    "if jax.process_index() == 0:\n",
    "    hooks += [\n",
    "        report_progress,\n",
    "        periodic_actions.Profile(logdir='./', num_profile_steps=5),\n",
    "    ]\n",
    "\n",
    "train_metrics = []\n",
    "\n",
    "with metric_writers.ensure_flushes(writer):\n",
    "    for step in tqdm(range(0, 100_000)):\n",
    "        with jax.profiler.StepTraceAnnotation('train', step_num=step):\n",
    "            rng, _ = jax.random.split(rng)\n",
    "            batch = jax.random.randint(rng, (3, 64), minval=0, maxval=70_000)\n",
    "            state, metrics = jit_train_step(\n",
    "                state, batch, learning_rate_fn, 0.0, dropout_rngs\n",
    "            )\n",
    "            train_metrics.append(metrics)\n",
    "\n",
    "        # Quick indication that training is happening.\n",
    "        logging.log_first_n(logging.INFO, 'Finished training step %d.', 5, step)\n",
    "        for h in hooks:\n",
    "            h(step)\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            with report_progress.timed('training_metrics'):\n",
    "                logging.info('Gathering training metrics.')\n",
    "                train_metrics = common_utils.stack_forest(train_metrics)\n",
    "                lr = train_metrics.pop('learning_rate').mean()\n",
    "                metrics_sums = jax.tree.map(jnp.sum, train_metrics)\n",
    "                denominator = metrics_sums.pop('denominator')\n",
    "                summary = jax.tree.map(lambda x: x / denominator, metrics_sums)  # pylint: disable=cell-var-from-loop\n",
    "                summary['learning_rate'] = lr\n",
    "                summary['perplexity'] = jnp.clip(jnp.exp(summary['loss']), max=1.0e4)\n",
    "                summary = {'train_' + k: v for k, v in summary.items()}\n",
    "                writer.write_scalars(step, summary)\n",
    "                train_metrics = []\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
