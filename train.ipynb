{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import TransformerLM\n",
    "from src.trainer import (\n",
    "    create_learning_rate_schedule,\n",
    "    setup_initial_state\n",
    ")\n",
    "from src.steps import train_step, eval_step\n",
    "import optax\n",
    "import jax\n",
    "from flax import nnx\n",
    "from jax import random\n",
    "from flax.training import common_utils\n",
    "from tqdm.auto import tqdm\n",
    "from src.confings import TransformerConfig, TrainerConfig\n",
    "from jax.experimental import mesh_utils\n",
    "from jax.sharding import Mesh, NamedSharding\n",
    "from jax.sharding import PartitionSpec as P\n",
    "from datasets import load_dataset\n",
    "import jax.numpy as jnp\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import jax.random as random\n",
    "from src.data import process_dataset\n",
    "import orbax.checkpoint as ocp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def const(config, key):\n",
    "    return TransformerLM(config, rngs=nnx.Rngs(params=key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfig()\n",
    "transformer_config = TransformerConfig(vocab_size=30_000, emb_dim=512, num_heads=8,\n",
    "                                        num_layers=3, qkv_dim=512, mlp_dim=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_devices = mesh_utils.create_device_mesh([1, 1, 1])\n",
    "mesh = Mesh(mesh_devices, ('data', 'fsdp', 'tensor'))\n",
    "\n",
    "learning_rate_fn = create_learning_rate_schedule(\n",
    "    learning_rate=trainer_config.learning_rate, warmup_steps=trainer_config.warmup_steps\n",
    ")\n",
    "tx = optax.adamw(learning_rate_fn, b1=0.9, b2=0.98, eps=1e-9,\n",
    "                  weight_decay=trainer_config.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_step = 1\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "rng, inference_rng = random.split(rng)\n",
    "dropout_rngs = rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, state_sharding = setup_initial_state(\n",
    "    const, tx, transformer_config, init_rng, mesh\n",
    ")\n",
    "\n",
    "data_sharding = NamedSharding(mesh, P(('data',)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_train_step = jax.jit(\n",
    "    train_step,\n",
    "    in_shardings=(state_sharding, data_sharding, None),\n",
    "    out_shardings=(state_sharding, None),\n",
    "    static_argnums=(2, 3),\n",
    "    donate_argnums=0\n",
    ")\n",
    "\n",
    "jit_eval_step = jax.jit(\n",
    "    eval_step,\n",
    "    in_shardings=(\n",
    "      state_sharding,\n",
    "      data_sharding,\n",
    "    ),  # type: ignore\n",
    "    out_shardings=None,  # type: ignore\n",
    "    static_argnums=(2,),\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hf = PreTrainedTokenizerFast.from_pretrained('ZurabDz/bpe_tokenizer_tmp', token='<>')\n",
    "# dataset = load_dataset('DKYoon/SlimPajama-6B', num_proc=6, split='train')\n",
    "dataset = load_dataset('roneneldan/TinyStories', num_proc=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = process_dataset(dataset['train'], tokenizer_hf, 14)\n",
    "eval_dataset = process_dataset(dataset['validation'], tokenizer_hf, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter_ds = train_dataset.iter(trainer_config.train_batch_size)\n",
    "eval_iter_ds = eval_dataset.iter(trainer_config.eval_batch_size)\n",
    "\n",
    "train_metrics = []\n",
    "eval_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"/home/penguin/Desktop/microlm/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_manager = ocp.CheckpointManager(\n",
    "    ocp.test_utils.erase_and_create_empty(checkpoint_dir),\n",
    "    options=ocp.CheckpointManagerOptions(\n",
    "        max_to_keep=5,\n",
    "        keep_checkpoints_without_metrics=False,\n",
    "        create=True,\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cf3c35a3004922a1a4698cffbf6e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary:  {'train_accuracy': Array(0.35568678, dtype=float32), 'train_loss': Array(3.3990588, dtype=float32), 'train_learning_rate': np.float32(0.0013243135), 'train_perplexity': Array(29.935911, dtype=float32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:[process=0][thread=async_save] Skipped cross-host ArrayMetadata validation because only one process is found: process_index=0.\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(start_step, min(50_000, len(train_dataset) // trainer_config.train_batch_size))):\n",
    "    batch = next(train_iter_ds)\n",
    "    jaxed_batch = jnp.array(batch['ids'])\n",
    "    state, metrics = jit_train_step(\n",
    "        state, jaxed_batch, learning_rate_fn, 0.0, dropout_rngs\n",
    "    )\n",
    "    train_metrics.append(metrics)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        train_metrics = common_utils.stack_forest(train_metrics)\n",
    "        lr = train_metrics.pop('learning_rate').mean()\n",
    "        metrics_sums = jax.tree.map(jnp.sum, train_metrics)\n",
    "        denominator = metrics_sums.pop('denominator')\n",
    "        summary = jax.tree.map(lambda x: x / denominator, metrics_sums)  # pylint: disable=cell-var-from-loop\n",
    "        summary['learning_rate'] = lr\n",
    "        summary['perplexity'] = jnp.clip(jnp.exp(summary['loss']), max=1.0e4)\n",
    "        summary = {'train_' + k: v for k, v in summary.items()}\n",
    "\n",
    "        train_metrics = []\n",
    "        print(\"summary: \", summary)\n",
    "\n",
    "        checkpoint_manager.save(\n",
    "            step, args=ocp.args.Composite(state=ocp.args.PyTreeSave(state))\n",
    "        )\n",
    "\n",
    "        break\n",
    "\n",
    "    # if (step + 1) % 100 == 0:\n",
    "    #     eval_iter_ds = eval_dataset.select(range(0, 500)).iter(trainer_config.eval_batch_size)\n",
    "    #     for batch in tqdm(eval_iter_ds, total=len(eval_dataset), leave=False):\n",
    "    #         metrics = jit_eval_step(state, jnp.array(batch['ids']))\n",
    "    #         eval_metrics.append(metrics)\n",
    "\n",
    "\n",
    "    #     eval_metrics = common_utils.stack_forest(eval_metrics)\n",
    "    #     eval_metrics_sums = jax.tree.map(jnp.sum, eval_metrics)\n",
    "    #     eval_denominator = eval_metrics_sums.pop('denominator')\n",
    "    #     eval_summary = jax.tree.map(\n",
    "    #         lambda x: x / eval_denominator,  # pylint: disable=cell-var-from-loop\n",
    "    #         eval_metrics_sums,\n",
    "    #     )\n",
    "    #     eval_summary['perplexity'] = jnp.clip(\n",
    "    #         jnp.exp(eval_summary['loss']), max=1.0e4\n",
    "    #       )\n",
    "        \n",
    "    #     print(\"eval_summary: \", {'eval_' + k: v for k, v in eval_summary.items()})\n",
    "    #     eval_metrics = []\n",
    "\n",
    "    # break\n",
    "    \n",
    "checkpoint_manager.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:CheckpointManagerOptions.read_only=True, setting save_interval_steps=0.\n",
      "WARNING:absl:CheckpointManagerOptions.read_only=True, setting create=False.\n",
      "WARNING:absl:Given directory is read only=/home/penguin/Desktop/microlm/output\n",
      "/home/penguin/micromamba/envs/microlm/lib/python3.12/site-packages/orbax/checkpoint/_src/serialization/type_handlers.py:1250: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with ocp.CheckpointManager(\n",
    "    checkpoint_dir, options=ocp.CheckpointManagerOptions(read_only=True)\n",
    ") as read_mgr:\n",
    "  restored = read_mgr.restore(\n",
    "      1000,\n",
    "      # pass in the model_state to restore the exact same State type\n",
    "      args=ocp.args.Composite(state=ocp.args.PyTreeRestore(item=state))\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Protocol, runtime_checkable\n",
    "from src import temperature_sampler\n",
    "import importlib\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class HasCache(Protocol):\n",
    "  def init_cache(self, input_shape, dtype = jnp.float32): ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = restored['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = nnx.merge(new_state.graphdef, new_state.params, new_state.keys, new_state.rest)\n",
    "inputs = jnp.array(tokenizer_hf([\"Little cat\"])['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.temperature_sampler' from '/home/penguin/Desktop/microlm/src/temperature_sampler.py'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(temperature_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(cgarciae): check how pytorch does this.\n",
    "for _path, m in module.iter_modules():\n",
    "    if isinstance(m, HasCache):\n",
    "        input_shape = (inputs.shape[0], transformer_config.max_len, transformer_config.emb_dim)\n",
    "        m.init_cache(input_shape, dtype=transformer_config.dtype)\n",
    "\n",
    "graphdef, params, cache, keys, rest = nnx.split(module, nnx.Param, nnx.Cache, nnx.RngKey, ...)\n",
    "\n",
    "def tokens_ids_to_logits(flat_ids, cache: nnx.State):\n",
    "    \"\"\"Token slice to logits from decoder model.\"\"\"\n",
    "    # --> [batch * beam, 1, vocab]\n",
    "    module = nnx.merge(graphdef, params, cache, keys, rest)\n",
    "    module.set_attributes(deterministic=True, decode=True)\n",
    "    logits = module(flat_ids, nnx.Rngs(0))\n",
    "    cache = nnx.state(module, nnx.Cache)\n",
    "    # Remove singleton sequence-length dimension:\n",
    "    # [batch, 1, vocab] --> [batch, vocab]\n",
    "    logits = logits.squeeze(axis=1)\n",
    "    return logits, cache\n",
    "\n",
    "# Using the above-defined single-step decoder function, run a\n",
    "# beam search over possible sequences given input encoding.\n",
    "seqs = temperature_sampler.temperature_sample(\n",
    "    jnp.pad(inputs, ((0, 0), (0, 64))),\n",
    "    cache,\n",
    "    tokens_ids_to_logits,\n",
    "    rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Little cat ran around and smiled. The dog was sad.\\n\\n\\n\\nHe smiled and said to the rabbit was so excited to help the dog was happy to help and said, so excited. He said, \"I want to have a big boy. He can play with the dog\\'s mommy. He was proud of'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_hf.decode(seqs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
